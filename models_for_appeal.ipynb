{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath=r'documents/Smth'\n",
    "only_documents=[f for f in listdir(mypath) if isfile(join(mypath,f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "good = 0\n",
    "bad = 0\n",
    "for doc in only_documents:\n",
    "    docxZip=zipfile.ZipFile(r'documents/Smth/'+doc)\n",
    "    textsXML=docxZip.read('word/document.xml')\n",
    "    xml_str=textsXML.decode('utf-8')\n",
    "    s = xml_str\n",
    "    proccess=1\n",
    "    needed_text_for_docx=[]\n",
    "    \n",
    "    while proccess:\n",
    "    sub_str=find_between(s, 'commentRangeStart', 'commentRangeEnd')\n",
    "    sub_str_final = find_between(sub_str, '<w:t>', '</w:t>')\n",
    "    if sub_str_final=='!':\n",
    "        sub_str_final=find_between(sub_str, '<w:t xml:space=\"preserve\">', '</w:t>')\n",
    "    if sub_str == '!':\n",
    "        break\n",
    "    s=s.replace(sub_str,'',1)\n",
    "    s=s.replace('commentRangeStartcommentRangeEnd', '', 1)\n",
    "    sub_str_final=sub_str_final.lstrip().rstrip()\n",
    "    needed_text_for_docx.append(sub_str_final)\n",
    "    \n",
    "    docxZip=zipfile.ZipFile(r'documents/Smth'+doc)\n",
    "    try:\n",
    "        textsXML=docxZip.read('word/comments.xml')\n",
    "    except KeyError:\n",
    "        continue\n",
    "    xml_str=textsXML.decode('utf-8')\n",
    "    \n",
    "    classes_for_docx=[]\n",
    "    s=xml_str\n",
    "    sub_str_all=''\n",
    "    proccess=1\n",
    "    while proccess:\n",
    "        sub_str_all=''\n",
    "        sub_str=find_between(s, '<w:comment w:', '/w:comment>')\n",
    "        if sub_str=='!':\n",
    "            break\n",
    "        s=s.replace(sub_str,'',1)\n",
    "        s=s.replace('<w:comment w:/w:comment>', '', 1)\n",
    "        while proccess:\n",
    "            sub_str_one=find_between(sub_str, '<w:t>', '</w:t>')\n",
    "            if sub_str_one == '!':\n",
    "                break\n",
    "            sub_str=sub_str.replace('<w:t>'+sub_str_one+'</w:t>', '', 1)\n",
    "            sub_str=sub_str.lstrip().rstrip()\n",
    "            sub_str_all+=sub_str_one\n",
    "        classes_for_docx.append(sub_str_all)\n",
    "        \n",
    "    if len(classes_for_docx) != len(needed_text_for_docx):\n",
    "        bad += 1\n",
    "        continue\n",
    "    else:\n",
    "        good += 1\n",
    "    text = getText(r'documents/Smth'+doc)\n",
    "    #text = text.replace('\\xad', '')\n",
    "    #text = text.replace('данн ые', 'данные')\n",
    "    #text = text.replace('\\xa0№\\xa0', ' № ')\n",
    "    \n",
    "    split_text = split_into_sentences_rus(text)\n",
    "    counts_of_comments = 0\n",
    "    for text in split_text:\n",
    "        checker='-' #Find entities\n",
    "        sentence=[] # for each sentence\n",
    "        tokens=[] # for each sentence\n",
    "        tags=[] # for each sentence\n",
    "        for index in range(counter_of_elements, len(needed_text_for_docx)):\n",
    "            checker='-'\n",
    "            if needed_text_for_docx[index] in text:\n",
    "                checker='+'\n",
    "                text_before=text[:text.find(needed_text_for_docx[index])].split()\n",
    "                if len(text_before) > 1:\n",
    "                    for i in range(len(text_before)):\n",
    "                        tokns.append(text_before[i])\n",
    "                        tags.append('O')\n",
    "                else:\n",
    "                    try:\n",
    "                        tokens.append(text_before[0])\n",
    "                        tags.append('O')\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                text = text[text.find(needed_text_for_docx[index]):]\n",
    "\n",
    "                comment = needed_text_for_docx[index].split()\n",
    "                if len(comment)==1:\n",
    "                    tokens.append(comment[i])\n",
    "                    tags.append('B-'+classes_for_docx[index])\n",
    "                else:\n",
    "                    for i in range(len(comment)):\n",
    "                        tokens.append(comment[i])\n",
    "                        if i == 0:\n",
    "                            tags.append('B-'+classes_for_docx[index])\n",
    "                        else:\n",
    "                            tags.append('I-'+classes_for_docx[index])\n",
    "                text = text.replace(needed_text_for_docx[index], '',1)\n",
    "            else:\n",
    "                if index==counter_of_comments:\n",
    "                    checker='-'\n",
    "                    break\n",
    "                else:\n",
    "                    checker='+'\n",
    "                    text=text.split()\n",
    "                    if len(text) > 1:\n",
    "                        for i in range(len(text)):\n",
    "                            tokens.append(text[i])\n",
    "                            tags.append('O')\n",
    "                    else:\n",
    "                        try:\n",
    "                            tokens.append(text[0])\n",
    "                            tags.append('O')\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "                    break\n",
    "        counter_of_comments = index\n",
    "        if checker == '-':\n",
    "            text=text.split()\n",
    "            if len(text) > 1:\n",
    "                for i in range(len(text)):\n",
    "                    tokens.append(text[i])\n",
    "                    tags.append('O')\n",
    "            else:\n",
    "                tokens.append(text[0])\n",
    "                tags.append('O')\n",
    "        sentence.append((tokens,tags))\n",
    "        sentences.append((tokens,tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i][0].append('.')\n",
    "    sentences[i][1].append('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
