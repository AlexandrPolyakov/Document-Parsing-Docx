{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import zipfile\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "\n",
    "SEED=42\n",
    "MODEL_PATH='model/'\n",
    "MODEL_FILE_NAME='ner_model.ckpt'\n",
    "\n",
    "def find_between(s,first,last):\n",
    "    try:\n",
    "        start=s.index(first)+len(first)\n",
    "        end=s.index(last,start)\n",
    "        if len(s[start:end]) >= 1:\n",
    "            return s[start:end]\n",
    "        else:\n",
    "            return '!'\n",
    "    except ValueError:\n",
    "        return '!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "docxZip=zipfile.ZipFile('ner_test.docx')\n",
    "textsXML=docxZip.read('word/document.xml')\n",
    "xml_str=textsXML.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=xml_str\n",
    "proccess=1\n",
    "needed_text_for_docx=[]\n",
    "while proccess:\n",
    "    sub_str=find_between(s, 'commentRangeStart', 'commentRangeEnd')\n",
    "    sub_str_final = find_between(sub_str, '<w:t>', '</w:t>')\n",
    "    if sub_str_final=='!':\n",
    "        sub_str_final=find_between(sub_str, '<w:t xml:space=\"preserve\">', '</w:t>')\n",
    "    if sub_str == '!':\n",
    "        break\n",
    "    s=s.replace(sub_str,'',1)\n",
    "    s=s.replace('commentRangeStartcommentRangeEnd', '', 1)\n",
    "    sub_str_final=sub_str_final.lstrip().rstrip()\n",
    "    needed_text_for_docx.append(sub_str_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text of comments in Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "docxZip=zipfile.ZipFile('ner_test.docx')\n",
    "textsXML=docxZip.read('word/comments.xml')\n",
    "xml_str=textsXML.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_for_docx=[]\n",
    "s=xml_str\n",
    "sub_str_all=''\n",
    "proccess=1\n",
    "while proccess:\n",
    "    sub_str_all=''\n",
    "    sub_str=find_between(s, '<w:comment w:', '/w:comment>')\n",
    "    if sub_str=='!':\n",
    "        break\n",
    "    s=s.replace(sub_str,'',1)\n",
    "    s=s.replace('<w:comment w:/w:comment>', '', 1)\n",
    "    while proccess:\n",
    "        sub_str_one=find_between(sub_str, '<w:t>', '</w:t>')\n",
    "        if sub_str_one == '!':\n",
    "            break\n",
    "        sub_str=sub_str.replace('<w:t>'+sub_str_one+'</w:t>', '', 1)\n",
    "        sub_str=sub_str.lstrip().rstrip()\n",
    "        sub_str_all+=sub_str_one\n",
    "    classes_for_docx.append(sub_str_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Перейдем теперь', 'Во-первых']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needed_text_for_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test1', 'Test2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_for_docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "def para2text(p):\n",
    "    rs=p._element.xpath('.//w:t')\n",
    "    return u\" \".join([r.text for r in rs])\n",
    "\n",
    "def getText(filename):\n",
    "    doc=docx.Document(filename)\n",
    "    fullText=[]\n",
    "    for para in doc.paragraphs:\n",
    "        #smartTag\n",
    "        #fullText.append(para2text[para])\n",
    "        #without smartTag\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document to sentences\n",
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "alphabets= \"([А-Яа-я])\"\n",
    "prefixes = \"(ст|Рф|Гз)[.]\"\n",
    "suffixes = \"(ст|Рф|Гз)\"\n",
    "starters = \"(ст|Рф|Гз)\"\n",
    "acronyms = \"([А-Я][.][А-Я][.](?:[А-Я][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences_rus(text):\n",
    "    digits = \"([0-9])\"\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text) \n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"'\" in text: text = text.replace(\".\"\",\"\".\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=getText('ner_test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text=split_into_sentences_rus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[] #all sentences\n",
    "counter_of_comments=0\n",
    "for text in split_text:\n",
    "    checker='-' #is it in sentence or not\n",
    "    sentence=[] #for every sentence\n",
    "    tokens=[] #for every sentence\n",
    "    tags=[] #for every sentence\n",
    "    for index in range(counter_of_comments, len(needed_text_for_docx)):\n",
    "        checker='-'\n",
    "        if needed_text_for_docx[index] in text: #write comments\n",
    "            checker='+'\n",
    "            text_before=text[:text.find(needed_text_for_docx[index])].split()\n",
    "            if len(text_before) > 1:\n",
    "                for i in range(len(text_before)):\n",
    "                    tokens.append(text_before[i])\n",
    "                    tags.append('O')\n",
    "            else:\n",
    "                try:\n",
    "                    tokens.append(text_before[0])\n",
    "                    tags.append('O')\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            text = text[text.find(needed_text_for_docx[index]):]\n",
    "            #append comment and text\n",
    "            comment = needed_text_for_docx[index].split()\n",
    "            if len(comment)==1:\n",
    "                tokens.append(comment[i])\n",
    "                tags.append('B-'+classes_for_docx[index])\n",
    "            else:\n",
    "                for i in range(len(comment)):\n",
    "                    tokens.append(comment[i])\n",
    "                    if i == 0:\n",
    "                        tags.append('B-'+classes_for_docx[index])\n",
    "                    else:\n",
    "                        tags.append('I-'+classes_for_docx[index])\n",
    "            text = text.replace(needed_text_for_docx[index], '',1)\n",
    "        else:\n",
    "            if index==counter_of_comments:\n",
    "                checker='-'\n",
    "                break\n",
    "            else: #add comment to the end\n",
    "                checker='+'\n",
    "                text=text.split()\n",
    "                if len(text) > 1:\n",
    "                    for i in range(len(text)):\n",
    "                        tokens.append(text[i])\n",
    "                        tags.append('O')\n",
    "                else:\n",
    "                    try:\n",
    "                        tokens.append(text[0])\n",
    "                        tags.append('O')\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                break\n",
    "    counter_of_comments = index\n",
    "    if checker == '-':\n",
    "        text=text.split()\n",
    "        if len(text) > 1:\n",
    "            for i in range(len(text)):\n",
    "                tokens.append(text[i])\n",
    "                tags.append('O')\n",
    "        else:\n",
    "            tokens.append(text[0])\n",
    "            tags.append('O')\n",
    "    sentence.append((tokens,tags))\n",
    "    sentences.append((tokens,tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add in the end\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i][0].append('.')\n",
    "    sentences[i][1].append('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Рассмотренные',\n",
       "   'ранее',\n",
       "   'когерентные',\n",
       "   'состояния',\n",
       "   'являются',\n",
       "   'очень',\n",
       "   'важным,',\n",
       "   'но,',\n",
       "   'тем',\n",
       "   'не',\n",
       "   'менее,',\n",
       "   'частным',\n",
       "   'случаем',\n",
       "   'из',\n",
       "   'класса',\n",
       "   'состояний,',\n",
       "   'которые',\n",
       "   'минимизируют',\n",
       "   'соотношение',\n",
       "   'неопределенностей.',\n",
       "   '.'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']),\n",
       " (['Действительно,',\n",
       "   'в',\n",
       "   'когерентном',\n",
       "   'состоянии',\n",
       "   'флуктуации',\n",
       "   'координаты',\n",
       "   'и',\n",
       "   'импульса',\n",
       "   'равны:',\n",
       "   'Перейдем',\n",
       "   'теперь',\n",
       "   'к',\n",
       "   'общему',\n",
       "   'случаю',\n",
       "   'и,',\n",
       "   'прежде',\n",
       "   'всего,',\n",
       "   'уточним',\n",
       "   'постановку',\n",
       "   'вопроса.',\n",
       "   '.'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Test1',\n",
       "   'I-Test1',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']),\n",
       " (['А',\n",
       "   'именно,',\n",
       "   'нас',\n",
       "   'будут',\n",
       "   'интересовать',\n",
       "   'состояния,',\n",
       "   'минимизирующие',\n",
       "   'соотношение',\n",
       "   'неопределенностей',\n",
       "   'для',\n",
       "   'двух',\n",
       "   'сопряженных',\n",
       "   'наблюдаемых,',\n",
       "   'операторы',\n",
       "   'которых',\n",
       "   'подчиняются',\n",
       "   'каноническому',\n",
       "   'коммутационному',\n",
       "   'соотношению.',\n",
       "   '.'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']),\n",
       " (['Это', 'обобщение', 'включает', 'два', 'существенных', 'момента.', '.'],\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'O'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
